{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a0fb77-970f-4757-bd51-8be8a3a69a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Random forest regression is a supervised learning algorithm and bagging technique that uses an ensemble learning method for regression in machine learning. The trees in random forests run in parallel, meaning there is no interaction between these trees while building the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba40f0-fbbf-470e-b815-d577ae8d6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2Random forests deals with the problem of overfitting by creating multiple trees, with each tree trained slightly differently so it overfits differently. Random forests is a classifier that combines a large number of decision trees. The decisions of each tree are then combined to make the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d2b89a-d104-4685-92b5-2b6ed02d2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Random forest is an ensemble learning algorithm that uses a collection of decision trees to make predictions. Each decision tree is trained on a different subset of the data, and the predictions of all the trees are averaged to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34dfbdd6-ac70-4639-9df5-af9aad926f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 In the case of a random forest, hyperparameters include the number of decision trees in the forest and the number of features considered by each tree when splitting a node. (The parameters of a random forest are the variables and thresholds used to split each node learned during training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60ac02e-cdaa-4f51-870e-6488d1aa412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Decision tree is a combination of decisions, and a random forest is a combination of many decision trees. Random forest is slow, but decision tree is fast and easy on large data, especially on regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb6d4fbc-d74b-4836-896d-6b80af5957e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6It reduces overfitting in decision trees and helps to improve the accuracy.\n",
    "#It is flexible to both classification and regression problems.\n",
    "#It works well with both categorical and continuous values.\n",
    "#It automates missing values present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3db89c-8bb8-4c28-85af-025ac5dd1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7The averaging makes a Random Forest better than a single Decision Tree hence improves its accuracy and reduces overfitting. A prediction from the Random Forest Regressor is an average of the predictions produced by the trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c5e25-1626-46d0-9801-abab7135b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
